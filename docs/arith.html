<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Preliminary: Arithmetic Coding</title>
     <link rel="icon" type="image/svg+xml" href="res/images/tess.svg">
    <link rel="stylesheet" href="./css/default.css">
    <link rel="stylesheet" href="./css/syntax.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header>
      <div class="logo">
        <a href="./">nbos.ca</a>
      </div>
      <nav>
        <a href="./">Posts</a>
        <a href="./hackage.html">Hackage</a>
        <a href="./contact.html">Contact</a>
      </nav>
    </header>

    <main role="main">
      <h1>Preliminary: Arithmetic Coding</h1>
      <article>
  <section class="header">
    Posted on October 13, 2025
    
    by Nathaniel Bos
    
  </section>
  <section>
    <p>The topic of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Arithmetic_coding">arithmetic
coding</a> is probably as
good an introduction to information theory as we are going to get. The
notions required to understand it cover all the basics and its
embodiment of the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">source coding
theorem</a>
makes it a useful building block for understanding <em>information</em> and
further concepts.</p>
<p>Here is my take on an explanation.</p>
<h2 id="symbol-by-symbol-coding">Symbol-by-symbol Coding</h2>
<p>The problem at hand is that of converting a sequence of symbols between
two alphabets. In practice, however, we only consider binary encodings,
where the target alphabet is of size 2: <span class="math inline">\(\{0,1\}\)</span>, but everything
presented here will equally apply to alphabets of larger sizes (e.g. 10:
<span class="math inline">\(\{0,1,2,3,4,5,6,7,8,9\}\)</span>) with respective <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/List_of_logarithmic_identities#Changing_the_base">changes of
base</a>
from 2 (to e.g. 10).</p>
<h3 id="block-coding">Block Coding</h3>
<p>If the source and target alphabets are of the same size, the conversion
can be made trivially by assigning a single target symbol to each source
symbol:</p>
<p><img src="res/arith/01AB.svg" /></p>
<p>So a sequence <span class="math inline">\(ABA\)</span> would encode into <span class="math inline">\('010'\)</span> and vice versa for
decoding.</p>
<p>Larger alphabets require assigning longer codes to each symbol. For a
source alphabet of 4 symbols:</p>
<p><img src="res/arith/ABCD.svg" /></p>
<p>each letter is assigned a 2-bit code. The same sequence <span class="math inline">\(ABA\)</span> would
encode into <span class="math inline">\('00~01~00'\)</span>.</p>
<p>Notice that, for alphabet sizes that are powers of 2, the length of
individual symbol codes is the <span class="math inline">\(\log_2\)</span> of the size. So 3-bit codes are
sufficient to support an alphabet of 8 symbols:</p>
<p><img src="res/arith/ABCDEFGH.svg" /></p>
<p>the same sequence <span class="math inline">\(ABA\)</span> would encode into <span class="math inline">\('000~001~000'\)</span>.</p>
<p>If the size of the alphabet is not a power of 2, there are codes that
aren’t assigned any symbol. For example, an alphabet of 26 symbols:</p>
<p><img src="res/arith/alphabet-0.svg" />
<img src="res/arith/alphabet-1.svg" /></p>
<p>This may be satisfactory for certain applications. If one has the added
constrait of prefering short
(i.e. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Lossless_compression">compressed</a>)
encodings, there are ways of making more <em>efficient</em> use of the
code-space.</p>
<h3 id="variable-length-coding">Variable Length Coding</h3>
<p>Consider assigning this extra space to symbols in a way that <em>shrinks</em>
the length of their assigned codes: if we assign both 5-bit codes
<span class="math inline">\('00000'\)</span> and <span class="math inline">\('00001'\)</span> to symbol <span class="math inline">\(A\)</span>, the last bit isn’t helping
distinguish between symbols anymore and can be dropped, leaving us with
the 4-bit code <span class="math inline">\('0000'\)</span>.</p>
<p>Further, if not all symbols are equally likely to be sampled, we can
preferentially absorb this extra code-space into symbols that are most
likely to occur, maximizing efficiency. For example, in English text,
letters <span class="math inline">\(\{A,E,I,N,O,T\}\)</span> usually occur more than others so assigning
more code-space to these symbols will result in shorter binary encodings
in the long run:</p>
<p><img src="res/arith/alphabet-fill-0.svg" />
<img src="res/arith/alphabet-fill-1.svg" /></p>
<p>Symbols being assigned 5-bits are effectively given <span class="math inline">\(\frac{1}{32}\)</span> of
the code-space, and the more likely 4-bit symbols are given
<span class="math inline">\(\frac{2}{32} = \frac{1}{16}\)</span> of the code-space. Encoding the string
<span class="math inline">\(ABA\)</span> according to this assignment would produce <span class="math inline">\('0000~00010~0000'\)</span>.</p>
<p>This notion of assigning smaller codes to more common symbols and longer
codes to less common symbols <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">can be
shown</a>
to produce codes of <em>minimal</em> length when the proportion of code-space
assigned to each symbol is equal to its proportion in the message
(i.e. string of symbols) to be encoded.</p>
<h3 id="huffman-coding">Huffman Coding</h3>
<p>Given a probability distribution over symbols, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman’s
algorithm</a> will
efficiently produce an assignments of codes to symbols which
consistently minimizes the length of encodings that are sampled from
that same probability distribution.</p>
<p>So if we consider the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical
distribution</a>
over letters as they typically appear in English text:</p>
<p><img src="res/arith/frequency-bars.svg" /></p>
<p><img src="res/arith/frequency-table.svg" /></p>
<p>Huffman’s algorithm will produce a “tree” corresponding to a code
assignment like so:</p>
<p><img src="res/arith/huffman-0.svg" />
<img src="res/arith/huffman-1.svg" /></p>
<p>where each symbol is alligned to a binary slice of the code space
reasonably close to its share in the distribution (unlabeled symbols, in
order, are <span class="math inline">\(K,Q,X,J,Z\)</span>). Encoding the string <span class="math inline">\(ABA\)</span> according to this
assignment would produce <span class="math inline">\('0000~111000~0000'\)</span>.</p>
<p>The length of individual symbol codes in this example range between 3
bits for the most frequent letters <span class="math inline">\(\{E,T\}\)</span> to 10 bits for the least
frequent <span class="math inline">\(\{J,Z\}\)</span>.</p>
<p>Because Huffman codes produce a fixed <em>assignment</em> between symbols and
codes, it can only give shares of code-space that are powers of
<span class="math inline">\(\frac{1}{2}\)</span>. The degree to which the Huffman tree’s distribution
“reasonably” models the true distribution can be shown graphically (true
in black, modeled in gray):</p>
<p><img src="res/arith/huffman-bars.svg" /></p>
<p>which consistently under- or over-shoots the share of code-space each
symbol takes to align them to powers of <span class="math inline">\(\frac{1}{2}\)</span> (e.g. <span class="math inline">\(0.125,
0.06125, 0.03215, 0.015625\)</span>, etc.)</p>
<p>As stated before, this assignment of codes to symbols is optimal,
insofar as it is an <em>assignment</em>, meaning that each symbol gets its own
fixed code. However, whenever the probability distribution doesn’t align
exactly along powers of <span class="math inline">\(\frac{1}{2}\)</span>, we can outperform methods like
Huffman by giving up this “alignment” of symbols to the code-space
entirely and working directly with the true distribution.</p>
<h2 id="arithmetic-coding">Arithmetic Coding</h2>
<p>Arithmetic coding is not a symbol-by-symbol coding scheme. Instead,
messages are directly assigned binary expansions that are precise enough
to uniquely identify them.</p>
<p>The same way every binary sequence corresponds to a finite interval of
code-space by nesting divisions by 2 in the unit interval:</p>
<p><img src="res/arith/binary.svg" /></p>
<p>So does every sequence of symbols from our alphabet correspond to a
finite interval of the unit space by nesting divisions of the space into
our distribution of symbols. For example, given a distribution over two
symbols <span class="math inline">\(\{A \mapsto 0.67, B \mapsto 0.33\}\)</span>:</p>
<p><img src="res/arith/ab.svg" /></p>
<p>To arithmetically encode a sequence of symbols given a distribution, it
suffices to find the first binary interval that fits entirely within the
interval determined by the sequence we want to encode, and vice versa
for decoding.</p>
<h3 id="example-1">Example 1</h3>
<p>To encode the sequence <span class="math inline">\(ABA\)</span> according to the above toy distribution:</p>
<p><img src="res/arith/ABA-arith-0.svg" /></p>
<p>We find that the code <span class="math inline">\('1000'\)</span> uniquely determines the sequence <span class="math inline">\(ABA\)</span> by
being the first binary sequence whose interval fits entirely within the
interval of the symbol sequence:</p>
<p><img src="res/arith/ABA-arith-1.svg" /></p>
<p>The exact way in which this mapping between the code- and symbol-space
is computed will vary by implementation with different trade-offs
between precision, memory requirements and types of probability
distribution.</p>
<h3 id="example-2">Example 2</h3>
<p>Any more realistic example becomes challenging to demonstrate
graphically.</p>
<p>Consider encoding the sequence of letters <span class="math inline">\(NOT\)</span> from a 26 letter
alphabet with associated probabilities. Below, message space is
separated between letters in alphabetical order and each truncation of
the space is highlighted in red in the previous space.</p>
<p><img src="res/arith/NOT-arith-0.svg" /></p>
<p>The distribution for the second symbol is nested within the interval of
the first symbol:</p>
<p><img src="res/arith/NOT-arith-1.svg" /></p>
<p><img src="res/arith/NOT-arith-2.svg" /></p>
<p>Again, the distribution for the third symbol is nested within the
interval for the first two:</p>
<p><img src="res/arith/NOT-arith-3.svg" /></p>
<p><img src="res/arith/NOT-arith-4.svg" /></p>
<p><img src="res/arith/NOT-arith-5.svg" /></p>
<p>We find the word <span class="math inline">\('NOT'\)</span> to be uniquely determined by <span class="math inline">\('100100110111'\)</span>,
the size of which is <span class="math inline">\(12\)</span> bits.</p>
<h2 id="information">Information</h2>
<p>By nesting distributions within symbol intervals, the proportion of the
unit interval taken by subsequent symbols is scaled by the width of the
interval they are nested in.</p>
<p>Therefore, the width of any message is the <strong>product</strong> of the widths
(i.e. probabilities) of its constituent symbols, which is equivalent to
assigning messages probabilities equal to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">probability of
independently
sampling</a>
each of its symbols in sequence:</p>
<p><span class="math display">\[P(\mathrm{\bf x}) = \prod_i{P(x_i)}\]</span></p>
<p>Knowing this width (i.e. probability), we know that the length of the
code’s interval must be contained within, meaning the arithmetic code
itself <em>cannot be shorter</em> than the number of times it takes to split
the unit interval in half to reach that width, i.e. the logarithm
base-<span class="math inline">\(\frac{1}{2}\)</span> of the probability, i.e. the negative logarithm
base-2:</p>
<p><span class="math display">\[\begin{align}\mathrm{len}(\mathrm{\bf x})_2 &amp;~\ge~~ \log_{\frac{1}{2}}\left(\prod_i{P(x_i)}\right)\\ &amp;~\ge -\log_2\left(\prod_i{P(x_i)}\right)\end{align}\]</span></p>
<p>Or simply:</p>
<p><span class="math display">\[\mathrm{len}(\mathrm{\bf x})_2 ~\ge -\log_2P(\mathrm{\bf x})\]</span></p>
<p>This lower-bound on the length of an encoding is precisely the
definition the
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Information_content">information</a> of an
event <span class="math inline">\(x\)</span> with probability <span class="math inline">\(P(x)\)</span>:</p>
<p><span class="math display">\[I(x) = -\log P(x)\]</span></p>
<p>It is also the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem">absolute theoretical
lower-bound</a>
to any compression method with respect to the given distribution with
probabilities <span class="math inline">\(P\)</span>.</p>
<p>Alongside this informational lower-bound, we can derive an <em>upper-bound</em>
for the length of an arithmetic encoding. Consider the level of binary
divisions immediately smaller than the width of the message (we round
the logarithm up to the next integer):</p>
<p><span class="math display">\[\mathrm{len}(\mathrm{\bf x})_2 ~\ge~ \left\lceil-\log_2P(\mathrm{\bf x})\right\rceil\]</span></p>
<p>Notice that, although it is possible that no binary interval immediately
smaller than the message’s interval fits within it if intervals are not
in perfect phase with each other (as in one of the examples above):</p>
<p><img src="res/arith/ABA-arith-phase.svg" /></p>
<p>Subdividing the space only once more is guaranteed to produce a binary
interval within the message interval. Since the arithmetic encoding is
defined as the <em>first</em> binary interval fully within the message’s
interval, we have the upper-bound:</p>
<p><span class="math display">\[\mathrm{len}(\mathrm{\bf x})_2 ~\le~ \left\lceil-\log_2P(\mathrm{\bf x})\right\rceil + 1\]</span></p>
<p>which makes an arithmetic encoding <em>at most</em> 1-bit longer than the
minimum it (and any other algorithm) can possibly achieve.</p>
<h2 id="eom">EOM</h2>
<p>A caveat to the above calculations is that for some distributions,
namely if the probability of some symbols in the distribution is more
than <span class="math inline">\(\left(\frac{1}{2}\right)\)</span>, then individual bits can fail to
discriminate between two successive levels of the nested symbol space.</p>
<p>For example, the interval of code <span class="math inline">\('00'\)</span> is the first binary interval to
be fully within the bounds of both <span class="math inline">\(AA\)</span> and <span class="math inline">\(AAA\)</span> using the distribution
<span class="math inline">\(\{A \mapsto 0.67, B \mapsto 0.33\}\)</span>:</p>
<p><img src="res/arith/AB-caveat.svg" /></p>
<p>The usually stated workaround to this issue is to include a special
end-of-message (<strong>EOM</strong>) symbol to the distribution with a probability
of <span class="math inline">\(\left(\frac{1}{n}\right)\)</span> where <span class="math inline">\(n\)</span> is the length of the message and
use it as a terminating symbol, resolving the ambiguity.</p>
<p>This slightly skews the distribution and incurs an overall informational
cost equivalent to encoding <span class="math inline">\(n\)</span> times <span class="math inline">\(\left(\frac{n}{n+1}\right)\)</span> and
<span class="math inline">\(\left(\frac{1}{n+1}\right)\)</span> once:</p>
<p><span class="math display">\[\begin{align}
	\Delta\mathrm{len}(\mathrm{\bf x})
	&amp;= -\log\left(\left(\frac{n}{n+1}\right)^n \cdot \frac{1}{n+1}\right)\\[6pt]
	&amp;= -n\log\left(\frac{n}{n+1}\right) - \log\left(\frac{1}{n+1}\right)\\[6pt]
	&amp;= -n\log(n) + n\log(n+1) + \log(n+1)\\[6pt]
	&amp;= (n+1)\log(n+1) - n\log n\\[6pt]
\end{align}\]</span></p>
<p>Which grows on the order of <span class="math inline">\(O(\log n)\)</span>.</p>
<p>I find this solution unsatisfactory as either (a) the <strong>EOM</strong> symbol’s
probability is not exactly <span class="math inline">\(\left(\frac{1}{n}\right)\)</span> making the code
more than <span class="math inline">\(\log n\)</span> longer, or (b) the probability is exactly
<span class="math inline">\(\left(\frac{1}{n}\right)\)</span> in which case the decoder already knows the
length of the message by way of the probability and the introduction of
an <strong>EOM</strong> symbol becomes unnecessary.</p>
<p>The more elegant alternative is to prepend messages with a binary
encoding of its length, letting the decoder know exactly where to stop
the recursion. A direct binary encoding of a positive integer has a
similar <span class="math inline">\(\lceil\log_2n\rceil\)</span> bits cost but simply pre-pending this code
to the message’s code makes it impossible to determine where the first
code stop and the second begins.</p>
<p>The solution is to either dedicate a <em>fixed length</em> (e.g. 32- or 64-bit)
number or for something more sophisticated, a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Universal_code_(data_compression)">universal prefix
code</a>
(e.g. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Elias_coding">Elias</a>) can be used
which have an unambiguous ending (the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Prefix_code">prefix
property</a>) and also take on
the order of <span class="math inline">\(O(\log n)\)</span> bits for any <span class="math inline">\(n \in \mathbb{N}\)</span>.</p>
<h2 id="adaptive-coding">Adaptive Coding</h2>
<p>So far, distributions have been assumed to remain unchanged over the
span of messages. This doesn’t have to be the case.</p>
<p>If instead sharing a single distribution, the encoder and decoder share
a <em>model</em> that updates its probability distribution over symbols after
each successive symbol, the likelihood of plausible messages can be
increased. These more likely messages are thus addressable by shorter
codes, increasing the achieved compression factor by however much better
the adaptive model is able to predict symbols than the fixed
distribution.</p>
<p>With adaptive arithmetic coding, details about different compression
techniques can be largely omitted, leaving only the predictive power of
underlying models (i.e. expected likelihood of messages) to be
discussed, which, in turn, is much more amenable to analysis.</p>
  </section>
  <footer>
    <a href="./">Return</a>
  </footer>
</article>

    </main>

  </body>
</html>
