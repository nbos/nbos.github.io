<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ways to Count Information</title>
    <link rel="icon" type="image/svg+xml" href="images/tess.svg">
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="./css/syntax.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1 class="post-title">Ways to Count Information</h1>
    <div class="post-metadata">
        Posted on March 23, 2025 by Nathaniel Bos
    </div>
    <div class="post-content">
        <p>This note exists mainly because I couldn’t find much about this online.</p>
<h2 id="information-from-probability">Information From Probability</h2>
<p>When compressing data with <a href="arith.html">arithmetic coding</a>, the length
of the produced code is equal to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Information_content">information
content</a> of the data
(rounded to the next integer) with respect to the probabilistic model
used. It takes 1 bit to encode a coin flip, 2.58496 to encode a dice
roll, 4.70044 for a letter of the alphabet. If you know the relative
frequency of letters in English, you only need about 4.18, if you know
the frequency of words, it goes down to about 2.62 per letter, and you
can imagine knowing grammar rules or even general knowledge would bring
it even further down.</p>
<p>In general, the formula for information content is</p>
<p><span class="math display">\[\mathrm{I}(x)=-\log p(x),\]</span></p>
<p>where <span class="math inline">\(p(x)\)</span> is the probability of <span class="math inline">\(x\)</span>. The information content of a
sequence of events or symbols is just the sum of the information
contents of each, just like the concatenated codes will have length
equal to the sum of individual codes.</p>
<h2 id="categorical-with-replacement">Categorical With Replacement</h2>
<p>For a sequence of <span class="math inline">\(N\)</span> symbols with individual frequencies <span class="math inline">\(n_0, n_1,
n_2, ...\)</span>, such that</p>
<p><span class="math display">\[\sum_i{n_i} = N,\]</span></p>
<p>the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical
model</a> giving
the shortest code length, i.e the highest
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> to the
data, is the one where the probability assigned to each symbol is
proportional to its frequency in the data (i.e. the
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE</a>). If
you work out the information content of the full sequence against its
categorical MLE, that comes out to</p>
<p><span class="math display">\[\begin{align}
\mathrm{I}(X) &amp;= -n_0 \log p(x_0) - n_1 \log p(x_1) - n_2 \log p(x_2) -\ ...\\[8pt]
&amp;= -n_0 \log \left(\frac{n_0}{N}\right) - n_1 \log \left(\frac{n_1}{N}\right) - n_2 \log \left(\frac{n_2}{N}\right) -\ ... \\[8pt]
&amp;= \left(\sum_i n_i\right) \log N -n_0 \log n_0 - n_1 \log n_1 - n_2 \log n_2 -\ ... \\[8pt]
&amp;= N \log N -n_0 \log n_0 - n_1 \log n_1 - n_2 \log n_2 -\ ...,
\ \end{align}\]</span></p>
<p>which conveniently isolates our integer parameters into distinct
terms. This formula, then, is the length of the arithmetic code of a
sequence if every symbol is predicted according to the MLE categorical
fitted to the sequence.</p>
<p>Note, however, that it’s easy to algorithmically improve on this code
length if the probabilistic model docks counts of symbols as they
appear, improving the likelihood of symbols down the sequence compared
to the “static” model.</p>
<h2 id="categorical-without-replacement">Categorical Without Replacement</h2>
<p>It can be proven that the information content of the categorical MLE
<em>without replacement</em> (i.e. that docks counts with each observation)
follows this formula:</p>
<p><span class="math display">\[\log(N!) -\log(n_0!) - \log(n_1!) - \log(n_2!) -\ ...,\]</span></p>
<p>Although the factorial is unkind to an algebraic treatment, we can show
this is true by induction from the end of the sequence towards the left.</p>
<p><span class="math inline">\(\texttt{Proof:}\)</span></p>
<!-- **Case N=0:** All parameters are zero and the code is also null. -->
<p><strong>Case N=1:</strong> For the sequence with 1 element, <span class="math inline">\(N = 1\)</span> and <span class="math inline">\(n_0 = 1\)</span>, so</p>
<p><span class="math display">\[\log(1!) - \log(1!) = 0,\]</span></p>
<p>as in the arithmetic code where the probability of the only symbol <span class="math inline">\(x_0\)</span>
is 1 so the code is empty.</p>
<p><strong>Case N+1:</strong> For an additional symbol <span class="math inline">\(x_i\)</span>, given that the length of
the code for the rest of the string is</p>
<p><span class="math display">\[\log(N!) -\log(n_0!) - \log(n_1!) - \log(n_2!) -\ ...,\]</span></p>
<p>for any count <span class="math inline">\(n_i\)</span> of that symbol in the rest of the string (including
0), the additional code for that symbol has length as a function of its
probability at that point:</p>
<p><span class="math display">\[\begin{align}\mathrm{I}(x_i)_{N+1} &amp;= -\log p(x_i)_{N+1} \\[8pt]
&amp;= -\log \left(\frac{n_i+1}{N+1}\right) \\[8pt]
&amp;= \log(N+1) -\log (n_i+1)\end{align}\]</span></p>
<p>which integrates into the factorials by the product property of
logarithms:</p>
<p><span class="math display">\[\begin{align}&amp;~ \log(N!) + \log(N+1) -\log(n_0!) -\ ...\ -\log(n_i!) - \log(n_i+1) -\ ... \\[8pt]
&amp;=\ \log(N! \cdot (N + 1)) -\log(n_0!) -\ ...\ - \log(n_i! \cdot (n_i + 1)) -\ ... \\[8pt]
&amp;=\ \log((N + 1)!) -\log(n_0!) -\ ...\ -\ \log((n_i + 1)!) -\ ...,\end{align}\]</span></p>
which is our formula with updated parameters.
<div style="text-align: right">
<span class="math inline">\(\blacksquare\)</span>
</div>
<h2 id="discussion">Discussion</h2>
<p>It is well known that <span class="math inline">\(n \log n\)</span> (i.e. <span class="math inline">\(\log(n^n)\)</span>) asymptotically
grows at the same rate as <span class="math inline">\(\log (n!)\)</span> and the former is used to
approximate the latter, as seen in <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stirling%27s_approximation">Stirling’s
approximation</a></p>
<p><span class="math display">\[\log(n!) = n\log n - n\log e + \frac{1}{2}\log(2\pi n) + O\left(\frac{1}{n}\right)\]</span></p>
<p><img src="count/stirling.png" /></p>
<p>or even <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Srinivasa_Ramanujan">Ramanujan</a>’s
more precise version:</p>
<p><span class="math display">\[\log(n!) = n\log n - n\log e+\frac{1}{6}\log(n(1+4n(1+2n)))+\frac{1}{2}\log \pi + O\left(\frac{1}{n^3}\right)\]</span></p>
<p><img src="count/ramanujan.png" /></p>
<p>but these additional terms correct a consistent and significant
difference between the two functions:</p>
<p><img src="count/difference.png" /></p>
<p>which definitely translates to the information content of categorical
MLE’s w/ vs. w/o replacement, but how much so is not
obvious. Experiments on random strings reveals that the w/ replacement
requires anywhere from 0.2% (on the longest strings with the smallest
alphabets) to 40% (on the smallest strings of mostly distinct symbols)
more information to encode the same string than doing it w/o replacement
(<a href="count/table.html">full table</a>):</p>
<pre><code>+-----+------+-----+-------------+------------+------------+-----------+
|   # |    N |   m | N log(N)... | log(N!)... |   (verif.) | delta (%) |
+=====+======+=====+=============+============+============+===========+
|   1 |  828 |   2 |    573.634  |   570.048  |   570.048  |  0.628937 |
+-----+------+-----+-------------+------------+------------+-----------+
|   2 | 4193 |  31 |  14378.1    | 14278.7    | 14278.7    |  0.696137 |
+-----+------+-----+-------------+------------+------------+-----------+
| ... |  ... | ... |      ...    |     ...    |     ...    |  ...      |
+-----+------+-----+-------------+------------+------------+-----------+
|  83 |  290 |   9 |    630.094  |   610.039  |   610.039  |  3.28758  |
+-----+------+-----+-------------+------------+------------+-----------+
|  84 | 3764 |   4 |   5217.54   |  5205.2    |  5205.2    |  0.236958 | (-)
+-----+------+-----+-------------+------------+------------+-----------+
|  85 |   14 |  11 |     32.7879 |    23.1118 |    23.1118 | 41.8667   | (+)
+-----+------+-----+-------------+------------+------------+-----------+
|  86 |  104 |   2 |     72.0104 |    69.4607 |    69.4607 |  3.67064  |
+-----+------+-----+-------------+------------+------------+-----------+
| ... |  ... | ... |      ...    |     ...    |     ...    |  ...      |</code></pre>
<p>where <span class="math inline">\(N\)</span> is the length of the string, <span class="math inline">\(m\)</span> is the size of the alphabet,
the information content is computed w/ and w/o replacement according to
the formulae and the w/o replacement value is confirmed through the sum
of the individual information of each symbol (simulating an adaptive
model), validating our proof. Lastly, the percentage difference
w.r.t. the information content w/o replacement.</p>
<p>What’s surprising is that there is no way for the order of the symbols
to affect the performance of a w/o replacement strategy. One would think
having all the instances of some symbol organized together (e.g. at the
beginning) would help an inference w/o replacement by getting them “out
of the way”, but the formula is simply not affected by the order of the
symbols in the string.</p>
<p>Finally, let’s address the <strong>combinatorial</strong> connection hinted so far by
our choice of vocabulary. Our formula for information content w/o
replacement is equal to the <span class="math inline">\(\log\)</span> of the so-called <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multinomial_theorem">multinomial
coefficient</a>:</p>
<p><span class="math display">\[\begin{align}
\mathrm{I}(X) &amp;= \log(N!) - \log(n_0!) - \log(n_1!) - \log(n_2!) -\ \ldots \\[8pt]
&amp;= \log \left(\frac {N!}{n_0!\ n_1!\ n_2!\ \cdots}\right) \\[8pt]
&amp;= \log {N \choose n_0,n_1,n_2,\ldots},\ \end{align}\]</span></p>
<p>which is the number of ways a sequence of <span class="math inline">\(N\)</span> objects with equivalence
classes of sizes <span class="math inline">\(n_0, n_1, n_2, ...\)</span>, where</p>
<p><span class="math display">\[\sum_i{n_i} = N.\]</span></p>
<p>So this whole derivation about probabilities and mutation of a
probabilistic model is equivalent to the <span class="math inline">\(\log\)</span> of the number of
possible strings given the same parameters. In the two cases where we
restrict the space as much as the parameters allow, we get the same
result.</p>
<h2 id="information-from-variety">Information From Variety</h2>
<p>This motivates an alternative definition of information, the one in term
of <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variety_(cybernetics)">variety</a>. Much
simpler in my opinion; there are no negations, no normalization, just</p>
<p><span class="math display">\[\mathrm{I}(X)= \log \left(V(X)\right),\]</span></p>
<p>where <span class="math inline">\(V(X)\)</span> is the <strong>variety</strong>, or number of states a system <span class="math inline">\(X\)</span> can
find itself in. In terms of the probabilistic definition, it’s as if
every state had equal probability:</p>
<p><span class="math display">\[\begin{align}\mathrm{I}(x) &amp;= -\log p(x) \\[5pt]
&amp;= -\log \left(\frac{1}{N}\right) \\[5pt]
&amp;= \log N.\end{align}\]</span></p>
<p>But even the probabilistic definition makes more sense in terms of
variety as long as the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Frequentist_probability">probability is a
ratio</a>:</p>
<p><span class="math display">\[\begin{align}\mathrm{I}(x_i) &amp;= -\log p(x_i) \\[5pt]
&amp;= -\log \left(\frac{n_i}{N}\right) \\[5pt]
&amp;= \log N - \log n_i.\end{align}\]</span></p>
<p>That is, the information content of a part (<span class="math inline">\(n_i\)</span>) in a whole (<span class="math inline">\(N\)</span>) is
the information to specify one among the whole (<span class="math inline">\(\log N\)</span>) minus the
information to specify one among the part (<span class="math inline">\(\log n_i\)</span>), like an interval
between <span class="math inline">\(n\)</span> and <span class="math inline">\(N\)</span> in <span class="math inline">\(\log\)</span>-space.</p>
    </div>
    <a href="./" class="return-link">← Return to Index</a>
</body>
</html>
