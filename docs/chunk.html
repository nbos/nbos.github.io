<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Information Theoretic Chunking</title>
     <link rel="icon" type="image/svg+xml" href="res/images/tess.svg">
    <link rel="stylesheet" href="./css/default.css">
    <link rel="stylesheet" href="./css/syntax.css">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </head>
  <body>
    <header>
      <div class="logo">
        <a href="./">nbos.ca</a>
      </div>
      <nav>
        <a href="./">Posts</a>
        <a href="./hackage.html">Hackage</a>
        <a href="./contact.html">Contact</a>
      </nav>
    </header>

    <main role="main">
      <h1>Information Theoretic Chunking</h1>
      <article>
  <section class="header">
    Posted on January  3, 2026
    
    by Nathaniel Bos
    
  </section>
  <section>
    <p>One of the more salient features of our cognition is the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Chunking_(psychology)">organization of
related parts into
wholes</a>.</p>
<p>In the absence of clear statistical principles guiding this drive to
<em>construct</em>, we attempt to reproduce similar structure by growing a
prediction model greedily in the direction of maximal compression.</p>
<p>For this, a serialization format is defined for text, based on
combinatorial objects <a href="count.html">equivalent</a> to such a model, which
produces codes of known lengths (i.e. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Information_content">information
content</a>), from which
we derive a loss function to guide deterministic construction of
dictionaries, of which we note their appearance and performance in
compression.</p>
<h2 id="note-on-overfitting">Note on Overfitting</h2>
<p>When modeling data with increasing numbers of parameters, the
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> of the
data can easily reach zero as the complexity of the data gets transferred
to the model instead of getting distilled into underlying features.</p>
<p>In machine learning, this is called
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> and is best
pictured by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Polynomial_regression">regression
lines</a>
(predictions) drifting away in wild and unlikely interpolations as more
parameters are added to the model:</p>
<p><img src="res/chunk/figs/overfit.svg" /></p>
<p>This behavior is undesirable in machine learning as it undermines the
<em>generalizability</em> of the model which is usually an important aspect of
the exercise.</p>
<p>In the context of compression, where the prediction of unobserved data
is not as central, the phenomenon is still pathological: as the
information of the data w.r.t. the model approaches zero, the
information required to describe the model measurably increases in
proportion.</p>
<p>While solutions to overfitting in machine learning almost always
implicate <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets">carving off sections of the
data</a>
to hide from the model only to be used later for measuring
generalizability, the context of compression allows a more direct
solution.</p>
<p>Instead of only minimizing the information (maximizing the
log-likelihood) of data <span class="math inline">\(\mathrm{\bf x}\)</span> given a model <span class="math inline">\(\theta\)</span>:
<span class="math display">\[I(\mathrm{\bf x} \mid \theta),\]</span> we minimize the information of <em>both</em>
the data and the model:</p>
<p><span class="math display">\[I(\mathrm{\bf x},\theta) = I(\mathrm{\bf x} \mid \theta) + I(\theta).\]</span></p>
<p>By measuring the information of the model together with the data’s, we
capture any information that merely transfers from data (given the
model) to the model and avoid an increase in parameters (model
complexity) that doesn’t decrease the <em>total</em> information.</p>
<!-- In the spirit of this principle we define a model for text data
that -->
<h2 id="serializing-combinatorial-objects">Serializing Combinatorial Objects</h2>
<p>As shown <a href="count.html#combinatorial-view">previously</a>,
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Combinatorics">counting</a> the
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Variety_(cybernetics)">variety</a> of
parametrized systems can produce simple closed formulas of their
information content (code length) w.r.t. an optimal encoder.</p>
<p>For example, the information in a sequence of <span class="math inline">\(N\)</span> symbols from an
alphabet of size <span class="math inline">\(m\)</span> with known counts
(a.k.a. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multiplicity_(mathematics)">multiplicities</a>)
<span class="math inline">\(n_0, n_1, ... n_{m-1}\)</span> using an optimal probabilistic model (i.e. an
updating
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical</a>
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE</a>) is
simply the <span class="math inline">\(\log\)</span> of the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Binomial_coefficient#Generalization_to_multinomials">multinomial
coefficient</a>
with those parameters:</p>
<p><span class="math display">\[\log {N \choose n_0,n_1,\ldots,n_{m-1}},\]</span></p>
<p>which is simply the number of ways to order a
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Permutation">multiset</a> of size <span class="math inline">\(N\)</span> with
multiplicities <span class="math inline">\(n_0,n_1,\ldots,n_{m-1}\)</span>
(<a href="count.html#categorical-without-replacement">proof</a>).</p>
<p>Further, given a total order
(e.g. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Lexicographic_order#Finite_subsets">lexicographic</a>)
on the summoned combinatorial object (here <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Permutation#Permutations_of_multisets">multiset
permutations</a>),
one can derive so-called <a href="(https://en.wikipedia.org/wiki/Combinatorial_number_system)">“ranking” and
“unranking”</a>
algorithms to map to and from natural numbers.</p>
<p>Interpreting these numbers in binary produces a serialization that is
equivalent in compression efficiency to entropy coding the sequence
symbol-by-symbol using derived probabilities.</p>
<p>We now define a serialization format for text data and its model using
such combinatorial codes.</p>
<h2 id="format-description">Format Description</h2>
<p>As stated, a sequence of <span class="math inline">\(N\)</span> symbols from an alphabet of size <span class="math inline">\(m\)</span> with
counts <span class="math inline">\(n_0,n_1,\ldots,n_{m-1}\)</span> can be encoded as a multiset permutation
with information:</p>
<p><span class="math display">\[\underbrace{\log {N \choose n_0,n_1,\ldots,n_{m-1}},
\vphantom{\prod_{\displaystyle i}}}
_{\displaystyle\mathrm{String} \vphantom{\prod}}\]</span></p>
<p>given that the counts are known.</p>
<p>The counts can themselves be encoded by counting the number of multisets
of total count <span class="math inline">\(N\)</span> and distinct symbols <span class="math inline">\(m\)</span>, according to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Multiset#Counting_multisets">multiset
coefficient</a>:</p>
<p><span class="math display">\[\left(\!\!\!{m \choose N}\!\!\!\right) = {m+N-1 \choose N} = \frac{(m+N-1)!}{N!\,(m-1)!},\]</span></p>
<p>or equivalently, using
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)">stars-and-bars</a>
(a permutation of <span class="math inline">\(N\)</span> stars and <span class="math inline">\(m-1\)</span> bars, giving the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Binomial_coefficient">binomial
coefficient</a>:</p>
<p><span class="math display">\[{N + m - 1 \choose m - 1} = \frac{(N+m-1)!}{N!\,(m-1)!}.\]</span></p>
<p>We prepend this encoding of counts to the encoding of the ordering of
the sequence:</p>
<p><span class="math display">\[\underbrace{\log {N + m - 1 \choose m - 1} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle n_0,n_1,\ldots,n_{m-1} \vphantom{\prod}}
+ \underbrace{\log {N \choose n_0,n_1,\ldots,n_{m-1}} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle\mathrm{String} \vphantom{\prod}}\]</span></p>
<p>This, in turn, assumes knowledge of parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(m\)</span>.</p>
<p>The length of the binary expansion of a natural number is on the order
of <span class="math inline">\(\log_2 n\)</span>, but without the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Prefix_code">prefix
property</a>, the length of such
an expansion (i.e. the cut-off point between the number’s code and the
rest of the code) is unknowable by a decoder. To address this, a
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Universal_code_(data_compression)">universal integer
code</a>
(e.g. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Elias_delta_coding">Elias</a>) is used
taking at most <span class="math inline">\(2\log_2 n\)</span> bits:</p>
<p><span class="math display">\[\underbrace{2\log m\vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle m \vphantom{\prod}}
+ \underbrace{2\log N\vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle N \vphantom{\prod}}
+ \underbrace{\log {N + m - 1 \choose m - 1} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle n_0,n_1,\ldots,n_{m-1} \vphantom{\prod}}
+ \underbrace{\log {N \choose n_0,n_1,\ldots,n_{m-1}} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle\mathrm{String} \vphantom{\prod}}.\]</span></p>
<p>Assuming the data is not random and contains repeating patterns, we can
reduce the size of this encoding by <em>chunking</em> symbols together into new
symbols (a.k.a. tokens), provided we append these definitions to the
encoding.</p>
<h3 id="inductive-constructions">Inductive Constructions</h3>
<p>For a sufficiently <em>universal</em> method of dictionary construction
(tokenization) that affords <em>incremental</em> <em>greedy</em> additions, we opt for
a dictionary defined inductively with <em>rules</em> consisting of two
previously defined symbols that concatenate to form a new <em>joint</em>
symbol.</p>
<p>By incrementally growing the dictionary and substituting instances where
the parts appear together in the string with joint symbols, the
exploration into the space of
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Word_n-gram_language_model"><span class="math inline">\(n\)</span>-grams</a>
will remain <em>sparse</em>, avoiding the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">combinatorial
explosion</a>
associated with assigning parameters to each possible combination of
symbols and the requirement to decide on a predetermined model size.</p>
<p>We start with an alphabet of the 256 bytes. The first construction rule
is therefore one among</p>
<p><span class="math display">\[256 \times 256\]</span></p>
<p>possible pairs. For the second rule introduction, the set of possible
pairs includes the previously introduced symbol and is one larger:</p>
<p><span class="math display">\[256 \times 256 \times 257 \times 257 \times \ldots\]</span></p>
<p>In general, for <span class="math inline">\(m\)</span> symbols, this has variety</p>
<p><span class="math display">\[\left(\frac{(m-1)!}{255!}\right)^2\]</span></p>
<p>which is information</p>
<p><span class="math display">\[\begin{align}
\log\left(\left(\frac{(m-1)!}{255!}\right)^2\right)
= 2\log\left(\frac{(m-1)!}{255!}\right).
\end{align}\]</span></p>
<p>We can also get away with encoding <span class="math inline">\(m - 256\)</span> instead of <span class="math inline">\(m\)</span> since it
will always be the case that <span class="math inline">\(m \geq 256\)</span>. The information content of
our entire encoding is therefore:</p>
<p><span class="math display">\[\begin{align}
I_{(m, \mathrm{\bf r}, N, \mathrm{\bf n}, \mathrm{\bf s})}
=~&amp;~ \underbrace{2\log (m-256)\vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle m \vphantom{\prod}}
+ \underbrace{2\log \left(\frac{(m-1)!}{255!}\right)\vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle\mathrm{{\bf r}: Rules} \vphantom{\prod}}
+ \underbrace{2\log N\vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle N \vphantom{\prod}}\\[10pt]
&amp;+ \underbrace{\log {N + m - 1 \choose m - 1} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle \mathrm{\bf n}: n_0,n_1,\ldots,n_{m-1} \vphantom{\prod}}
+ \underbrace{\log {N \choose n_0,n_1,\ldots,n_{m-1}} \vphantom{\prod_{\displaystyle i}}}
	_{\displaystyle\mathrm{{\bf s}: String} \vphantom{\prod}}
\end{align}\]</span></p>
<p>which is only a function of dictionary size <span class="math inline">\(m\)</span> and counts vector
<span class="math inline">\(\mathrm{\bf n}: n_0, n_1, ..., n_{m-1}\)</span> since <span class="math inline">\(N = \sum\mathrm{\bf n}\)</span>.</p>
<h2 id="loss-function">Loss Function</h2>
<p>Evaluating the above formula in whole for each possible addition to the
model to determine the optimal next step is logically what we want to
achieve but computationally excessive.</p>
<p>The terms for the length of the encoding of <span class="math inline">\(m\)</span> (dictionary size) and
<span class="math inline">\(\mathrm{\bf r}\)</span> (dictionary) are constant in information regardless of
<em>which</em> symbols are chosen for the next construction. While they
contribute in determining when the program stops (when the total length
stops decreasing), they can be dropped when sorting candidate
joints. Same goes for the encoding of parameter <span class="math inline">\(N\)</span> which is mostly
constant between introductions and at worse only a few bits in size at
specific steps between powers of two, which at most one will be crossed
as the maximum change <span class="math inline">\(N\)</span> can take is to get halved if every consecutive
pair in the working string gets constructed into a new symbol.</p>
<p>For the introduction of a joint symbol with count <span class="math inline">\(n_{01}\)</span>, constructed
from symbols with counts <span class="math inline">\((n_0,n_1)\)</span>, the main terms that affect the
total information are the term for parameters <span class="math inline">\(\mathrm{\bf n}\)</span> (the
counts):</p>
<p><span class="math display">\[\begin{align}
\Delta I_{\mathrm{\bf n}}
&amp;= I_{\mathrm{\bf n}'} - I_{\mathrm{\bf n}}\\[5pt]
&amp;= \log {N - n_{01} + m \choose m} - \log {N + m - 1 \choose m - 1}\\[5pt]
&amp;= \log \left(\frac{(N - n_{01} + m)!}{m!\,(N - n_{01})!}\right)
	- \log \left(\frac{(N + m - 1)!}{(m - 1)!\,N!}\right) \\[5pt]
&amp;= \log \left(\frac{(N - n_{01} + m)!\,(m - 1)!\,N!}
	{(N + m - 1)!\,m!\,(N - n_{01})!}\right) \\[5pt]
&amp;= \log \left(\frac{(N - n_{01} + m)!\,N!}
	{(N + m - 1)!\,m\,(N - n_{01})!}\right) \\[5pt]
\end{align}\]</span></p>
<!-- The equivalent expansion of the multinomial coefficient for the length -->
<!-- of the multiset permutation has on the order of $m$ terms but most -->
<!-- cancel out leaving only those implicated in the rule $(s_0, s_1) \to -->
<!-- s_{01}$: -->
<p>and the term for the ordering of the string:</p>
<p><span class="math display">\[\begin{align}
\Delta I_\mathrm{\bf s}
&amp;= I_{\mathrm{\bf s}'} - I_\mathrm{\bf s}\\[5pt]
&amp;= \log {N - n_{01} \choose n_0 - n_{01}, n_1 - n_{01},\ldots,n_{m-1}, n_{01}}
	- \log {N \choose n_0,n_1,\ldots,n_{m-1}}\\[5pt]
&amp;= \begin{cases}
	\log\left(\displaystyle \frac{(N - n_{01})!\,n_0!}
	{N!\,(n_0 - 2n_{01})!\,n_{01}!} \right)
	&amp; \text{when } s_0 = s_1 \\[5pt]
	\log\left(\displaystyle \frac{(N - n_{01})!\,n_0!\,n_1!}
	{N!\,(n_0 - n_{01})!\,(n_1 - n_{01})!\,n_{01}!} \right)
	&amp; \text{when } s_0 \neq s_1
	\end{cases} \\[5pt]
&amp;= \log \left( \frac{(N - n_{01})!\,n_0!}{N!\,n_{01}!} \right) + \begin{cases}
	\log \left(\frac{\displaystyle 1}{\displaystyle (n_0 - 2n_{01})!} \right)
	&amp; \text{when } s_0 = s_1 \\
	\log \left(\frac{\displaystyle n_1!}
	{\displaystyle (n_0 - n_{01})!\,(n_1 - n_{01})!} \right)
	&amp; \text{when } s_0 \neq s_1
	\end{cases}
\end{align}\]</span></p>
<p>$$$$</p>
<p>Together, some additional factors cancel out:</p>
<div id="loss-formula">
<p><span class="math display">\[\begin{align}
\Delta I_{(\mathrm{\bf n},\mathrm{\bf s})}
&amp;= \Delta I_{\mathrm{\bf n}} + \Delta I_\mathrm{\bf s} \\[5pt]
&amp;= \log \left(\frac{(N - n_{01} + m)!\,N!}
	{(N + m - 1)!\,m\,(N - n_{01})!}\right)
	+ \log \left( \frac{(N - n_{01})!\,n_0!}{N!\,n_{01}!} \right) \\[5pt]
&amp; ~~~~ + \begin{cases}
	\log \left(\frac{\displaystyle 1}{\displaystyle (n_0 - 2n_{01})!} \right)
	&amp; \text{when } s_0 = s_1 \\
	\log \left(\frac{\displaystyle n_1!}
	{\displaystyle (n_0 - n_{01})!\,(n_1 - n_{01})!} \right)
	&amp; \text{when } s_0 \neq s_1
	\end{cases} \\[5pt]
&amp;= \log \left(\frac{(N - n_{01} + m)!\,n_0!}
	{(N + m - 1)!\,m\,n_{01}!} \right) + \begin{cases}
	\log \left(\frac{\displaystyle 1}{\displaystyle (n_0 - 2n_{01})!} \right)
	&amp; \text{when } s_0 = s_1 \\
	\log \left(\frac{\displaystyle n_1!}
	{\displaystyle (n_0 - n_{01})!\,(n_1 - n_{01})!} \right)
	&amp; \text{when } s_0 \neq s_1
	\end{cases}
\end{align}\]</span></p>
</div>
<p>which, by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/List_of_logarithmic_identities#Using_simpler_operations">logarithmic
identity</a>,
can be computed in an efficient and numerically stable way with an
implementation of the
<a target="_blank" rel="noopener" href="https://hackage.haskell.org/package/math-functions-0.3.4.4/docs/Numeric-SpecFunctions.html#v:logFactorial">log-factorial</a>
using the expansion</p>
<p><span class="math display">\[\log((N - n_{01} + m)!) + \log(n_0!) - \log((N + m - 1)!)~-~...\]</span></p>
<p>$$$$</p>
<p>Finding the pair of symbols with counts <span class="math inline">\((n_0,n_1)\)</span> and joint count
<span class="math inline">\(n_{01}\)</span> which <strong>minimizes</strong> this function is sufficient to find the
rule to introduce which locally maximizes the compressibility of the
entire encoding.</p>
<p>As long as this loss—together with <span class="math inline">\(N\)</span>’s term—offsets the increase
in code length from terms <span class="math inline">\(\{m,\mathrm{\bf r}\}\)</span> incurred by an
introduction, we grow the dictionary (rule set).</p>
<h2 id="implementation">Implementation</h2>
<p>We implement a greedy algorithm that, given a text file, repeatedly
combines the pair of symbols that brings the greatest reduction in the
overall information content until no pair produces a loss below zero. We
log the introduced construction rules and information content at each
step.</p>
<p>The program is written in Haskell:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nbos/diagram_">Source (GitHub)</a></li>
</ul>
<h3 id="optimizations">Optimizations</h3>
<p>A number of additional optimizations were required to run the program on
inputs of significant length in a reasonable amount of time.</p>
<p>They mostly consist of maintaining different partial values required in
the selection of optimal introductions.</p>
<h4 id="counts-bookkeeping">Counts Bookkeeping</h4>
<p>First, as most of the symbol counts <span class="math inline">\(n_0,n_1,\ldots,n_{m-1}\)</span> stay
constant between rule introductions, the count vector (<span class="math inline">\(O(m)\)</span> space) is
obviously preserved between iterations and only those affected by the
introduced rule (the counts of the parts and the count of the new
symbol) get modified. This can be achieved in <span class="math inline">\(O(1)\)</span> time with a mutable
dynamic array.</p>
<h4 id="joint-counts-bookkeeping">Joint Counts Bookkeeping</h4>
<p>Similarly, the joint counts (number of times each pair of symbols appear
together) vary only slightly between iterations. The whole joint count
map (<span class="math inline">\(O(m^2)\)</span> space) is therefore preserved as each is a potential
candidate for the next rule.</p>
<p>When rewriting the string with the introduced rule <span class="math inline">\((s_0,s_1) \mapsto
s_{01}\)</span>, symbols occurring immediately before <span class="math inline">\(s_0\)</span> and immediately after
<span class="math inline">\(s_1\)</span> are subject to have their joint count with the respective part
decremented and the joint count with <span class="math inline">\(s_{01}\)</span> incremented.</p>
<!-- At worst, this has the same complexity as re-counting all joint
counts, but in practice where -->
<h4 id="joint-positions-bookkeeping">Joint Positions Bookkeeping</h4>
<p>With the above optimizations, the program can handle strings of
thousands (KB) up to a million (MB) symbols.</p>
<p>At this point, the operation taking by far most of the run time on large
strings is the <span class="math inline">\(O(N)\)</span> pass over the input required to</p>
<ol type="1">
<li><p>rewrite joint occurrences of <span class="math inline">\((s_0,s_1)\)</span> into <span class="math inline">\(s_{01}\)</span> and</p></li>
<li><p>update joint counts at sites where <span class="math inline">\(s_{01}\)</span> gets written</p></li>
</ol>
<p>The cost of this operation is especially noticeable after the first few
symbols have been introduced and joint counts—for a sample of natural
text—fall sharply following a
<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pareto_distribution">Pareto</a>-like drop
into a very long tail. The algorithm then spends most of its time
performing uneventful <span class="math inline">\(O(N)\)</span> scans of the working string, looking for
the few locations where the joint it has already decided to introduce
appears.</p>
<p>The obvious alternative to this is to store the set of construction
sites (as indexes) for each candidate joint and use a string
representation that allows <span class="math inline">\(O(1)\)</span> random access at those locations when
a rule is introduced.</p>
<p>Because the act of rewriting turns two symbols into one symbol, a vector
of symbols traditionally indexed either accumulates gaps of arbitrary
length over time, hindering access, or requires on the order of <span class="math inline">\(O(N)\)</span>
rewrites per introduction to close them.</p>
<p>The solution is to use a doubly linked list and the permanent memory
addresses (or something equivalent) of nodes as indexes allowing both
random access and closing gaps in constant time without affecting
indexes down the line.</p>
<p>In practice, this incurs significant time overhead at the beginning of
the execution of the program, but pays for itself many times over by
accelerating the long tail of the execution.</p>
<h4 id="joint-loss-bookkeeping">Joint Loss Bookkeeping</h4>
<p>The next operations which appropriates the bulk of the run-time on large
inputs is the evaluation and sorting of all joints according to the
<a href="#loss-formula">loss function</a>.</p>
<p>We can see this becomes severe when the size of the dictionary (<span class="math inline">\(m\)</span>) is
large as the number of possible joints grows on the order of
<span class="math display">\[O(\min(m^2,N)).\]</span></p>
<p>Examining our loss formula:</p>
<p><span class="math display">\[\Delta I_{(\mathrm{\bf n},\mathrm{\bf s})} =
\log \left( \frac
	{(N - n_{01} + m)!\,n_0!}
	{(N + m - 1)!\,m\,n_{01}!}
\right)
+ \begin{cases} \log  \left( \frac
	{\displaystyle 1}
	{\displaystyle (n_0 - 2n_{01})!}
\right) &amp; \text{when } s_0 = s_1 \\
\log \left( \frac
	{\displaystyle n_1!}
	{\displaystyle (n_0 - n_{01})!\,(n_1 - n_{01})!}
\right) &amp; \text{when } s_0 \neq s_1,
\end{cases}\]</span></p>
<p>splitting factors according to which term they originally came from</p>
<p><span class="math display">\[\Delta I_{(\mathrm{\bf n},\mathrm{\bf s})} =
\Delta I_\mathrm{\bf n}' + \Delta I_\mathrm{\bf s}'\]</span></p>
<p>we have a loss on the encoding of the counts:</p>
<p><span class="math display">\[\Delta I_\mathrm{\bf n}' =
	\log \left( \frac
		{(N - n_{01} + m)!}
		{(N + m - 1)!\,m}
	\right),
\]</span>
which is</p>
<ol type="1">
<li>always negative, given <span class="math inline">\(N,m,n_{01} &gt; 1\)</span></li>
<li>only depends on <span class="math inline">\(N\)</span>, <span class="math inline">\(m\)</span>, and <span class="math inline">\(n_{01}\)</span>, and</li>
<li>for a given <span class="math inline">\(N\)</span> and <span class="math inline">\(m\)</span>, minimal when <span class="math inline">\(n_{01}\)</span> is maximal,</li>
</ol>
<p>$$$$</p>
<p>and one on the encoding of the ordering of the string:</p>
<p><span class="math display">\[\Delta I_\mathrm{\bf s}' =
\begin{cases}
	\log \left( \frac
		{ \displaystyle n_0! }
		{ \displaystyle n_{01}! \, (n_0 - 2n_{01})! }
	\right)  &amp; \text{when } s_0 = s_1 \\
	\log \left( \frac
		{ \displaystyle n_0! \, n_1! }
		{ \displaystyle n_{01}! \, (n_0 - n_{01})! \, (n_1 - n_{01})! }
	\right) &amp; \text{when } s_0 \neq s_1,
\end{cases}
\]</span>
which is</p>
<ol type="1">
<li>always positive since it can be rewritten as</li>
</ol>
<p><span class="math display">\[\Delta I_\mathrm{\bf s}' =
\begin{cases}
	\log \left( \displaystyle
		{ n_0 \choose 2n_{01}} \cdot (2n_{01})!
	\right)  &amp; \text{when } s_0 = s_1 \\
	\log \left( \displaystyle
	    { n_0 \choose n_{01}} \cdot { n_1 \choose n_{01}} \cdot n_{01}!
	\right) &amp; \text{when } s_0 \neq s_1,
\end{cases}
\]</span></p>
<ol start="2" type="1">
<li>only depends on <span class="math inline">\(n_0\)</span>, <span class="math inline">\(n_1\)</span>, and <span class="math inline">\(n_{01}\)</span>, and</li>
<li>is minimal given <span class="math inline">\(n_{01}\)</span> when <span class="math inline">\(n_0\)</span> and <span class="math inline">\(n_1\)</span> are also minimal,
which is
<span class="math display">\[\mathrm{argmin}~\Delta I_\mathrm{\bf s}'(n_0,n_1 \mid n_{01}) =
 	\begin{cases}
 		n_0 := 2n_{01}  &amp; \text{when } s_0 = s_1 \\
 		(n_0,n_1) := (n_{01},n_{01}) &amp; \text{when } s_0 \neq s_1,
 	\end{cases}
 \]</span></li>
</ol>
<p>$$$$</p>
<p>Given these bounds, we can restrict our search for the minimal value of
<span class="math inline">\(\Delta I_{(\mathrm{\bf n},\mathrm{\bf s})}\)</span> on a certain subset of the
highest values of <span class="math inline">\(n_{01}\)</span>.</p>
<p>The chosen strategy was to sort joints first according to the joint
count <span class="math inline">\(n_{01}\)</span> and—for joints with the same value of
<span class="math inline">\(n_{01}\)</span>—according to the value of <span class="math inline">\(\Delta I_\mathrm{\bf s}'\)</span>. These
indexes remain constant between rule introductions where <span class="math inline">\(N\)</span> and <span class="math inline">\(m\)</span>
necessarily change value (which affects all loss calculations), but most
counts and joint counts remain constant.</p>
<p>Then, at each iteration, given values for <span class="math inline">\(N\)</span> and <span class="math inline">\(m\)</span>, we traverse the
joints along the spine of <span class="math inline">\(n_{01}\)</span>’s, from high to low, computing the
lowest values available for <span class="math inline">\(\Delta I_\mathrm{\bf n}'(N,m,n_{01})\)</span> in
order, and for each, add it to the minimal value of <span class="math inline">\(\Delta
I_\mathrm{\bf s}'(n_0,n_1 \mid n_{01})\)</span> (as indexed) and interrupt the
traversal when</p>
<p><span class="math display">\[\begin{align}
\text{best so far} &lt;~&amp; \Delta I_\mathrm{\bf n}'(N,m,n_{01}) \\
&amp;\!+~ \mathrm{argmin}~\Delta I_\mathrm{\bf s}'(n_0,n_1 \mid n_{01}).
\end{align}\]</span></p>
<p>This way, retrieving the joint with the minimal loss is almost instant
at the cost of having to maintain the indexes of joints for which any of
<span class="math inline">\(n_0\)</span>, <span class="math inline">\(n_1\)</span>, or <span class="math inline">\(n_{01}\)</span> gets changed after each introduction.</p>
<h2 id="results">Results</h2>
<p>The dataset of choice is a <a target="_blank" rel="noopener" href="https://mattmahoney.net/dc/textdata.html">2006 snapshot of the English
Wikipedia</a> (XML format),
following the example of the <a target="_blank" rel="noopener" href="https://mattmahoney.net/dc/textdata.html">Large Text Compression
Benchmark</a> and
<a target="_blank" rel="noopener" href="http://prize.hutter1.net/">Hutter prize</a>.</p>
<p>The first <span class="math inline">\(10^9\)</span> bytes of the snapshot are referred to as
<code>enwik9</code>. Truncations of smaller magnitudes are named accordingly:</p>
<p><img src="res/chunk/figs/datasets.svg" /></p>
<p>$$$$</p>
<p>For different sized inputs, we show the evolution of the total code
length (information) divided among the different components (terms) of
the encoding (formula) as the dictionary grows. The contributions of the
encodings of parameters <span class="math inline">\(m\)</span> and <span class="math inline">\(N\)</span> are too small to be noticeable and
left out. The size of the encoding at <span class="math inline">\(m=256\)</span> (empty dictionary) is
indicated with a marker on the Y axis:</p>
<p><img src="res/chunk/plots/codelen-stacked/enwik4-codelen-stacked.svg" /></p>
<p><img src="res/chunk/plots/codelen-stacked/enwik5-codelen-stacked.svg" /></p>
<p><img src="res/chunk/plots/codelen-stacked/enwik6-codelen-stacked.svg" /></p>
<p><img src="res/chunk/plots/codelen-stacked/enwik7-codelen-stacked.svg" /></p>
<p>As one would expect the bulk of the gain in compressibility occurs with
the first few introduced symbols and tapers out as introductions produce
fewer modifications to the string. We also notice that greater
compressibility is achieved with greater inputs (and larger
dictionaries).</p>
<p>We can compare the compressibility, or “information density” between
scales by computing a compression <em>factor</em> at each point in the model’s
evolution:</p>
<p><span class="math display">\[\text{compression factor} = \frac{\text{original size}}
{\text{compressed size}}.\]</span></p>
<p>A compression factor slightly above 1.5 is achieved across the board
with an empty dictionary simply by virtue of the combinatorial
encoding. Final compression factors are marked on the Y axis. The X axis
is displayed in log-scale:</p>
<div id="factors">
<p><img src="res/chunk/plots/factors.svg" /></p>
</div>
<p>For a given number of symbols, greater factors are achieved by smaller
sets, probably due to the reduced variance of a smaller dataset, but
larger inputs are amenable to greater factors in the long run, as the
large string encoding term (<span class="math inline">\(\mathrm{\bf s}\)</span> decreasing) can support
rule introductions for longer (<span class="math inline">\(\mathrm{\bf r}\)</span> and <span class="math inline">\(\mathrm{\bf n}\)</span>
increasing) before they begin to outweigh the reduction on <span class="math inline">\(\mathrm{\bf
s}\)</span>, triggering termination.</p>
<p>Ultimately, exponentially increasing input sizes translate to roughly
exponentially increasing dictionary sizes (and running time), and
<em>linearly</em> increasing compression factors.</p>
<p>Full outputs (CSV): <a href="res/chunk/out/enwik4.csv"><code>enwik4</code></a>,
<a href="res/chunk/out/enwik5.csv"><code>enwik5</code></a>,
<a href="res/chunk/out/enwik6.csv"><code>enwik6</code></a>,
<a href="res/chunk/out/enwik7.csv"><code>enwik7</code></a>.</p>
<h3 id="appearance">Appearance</h3>
<p>Chunks produced for <code>enwik</code> datasets are a mix of English morphemes,
words and phrases as well as markup strings from Wikipedia’s XML schema:</p>
<pre><code>256:  &quot;]&quot;     +  &quot;]&quot;    ==&gt;  &quot;]]&quot;
257:  &quot;[&quot;     +  &quot;[&quot;    ==&gt;  &quot;[[&quot;
258:  &quot;t&quot;     +  &quot;h&quot;    ==&gt;  &quot;th&quot;
259:  &quot;th&quot;    +  &quot;e&quot;    ==&gt;  &quot;the&quot;
260:  &quot;,&quot;     +  &quot; &quot;    ==&gt;  &quot;, &quot;
261:  &quot;'&quot;     +  &quot;'&quot;    ==&gt;  &quot;''&quot;
262:  &quot; &quot;     +  &quot;the&quot;  ==&gt;  &quot; the&quot;
263:  &quot; the&quot;  +  &quot; &quot;    ==&gt;  &quot; the &quot;
264:  &quot;\n&quot;    +  &quot;*&quot;    ==&gt;  &quot;\n*&quot;
265:  &quot;q&quot;     +  &quot;u&quot;    ==&gt;  &quot;qu&quot;
266:  &quot;&amp;&quot;     +  &quot;qu&quot;   ==&gt;  &quot;&amp;qu&quot;
267:  &quot;i&quot;     +  &quot;n&quot;    ==&gt;  &quot;in&quot;
268:  &quot;a&quot;     +  &quot;n&quot;    ==&gt;  &quot;an&quot;
269:  &quot;o&quot;     +  &quot;n&quot;    ==&gt;  &quot;on&quot;
270:  &quot;an&quot;    +  &quot;d&quot;    ==&gt;  &quot;and&quot;
271:  &quot;o&quot;     +  &quot;f&quot;    ==&gt;  &quot;of&quot;
272:  &quot; &quot;     +  &quot;of&quot;   ==&gt;  &quot; of&quot;
273:  &quot;&gt;&quot;     +  &quot;\n&quot;   ==&gt;  &quot;&gt;\n&quot;
274:  &quot;in&quot;    +  &quot;g&quot;    ==&gt;  &quot;ing&quot;
275:  &quot;t&quot;     +  &quot;;&quot;    ==&gt;  &quot;t;&quot;
276:  &quot;e&quot;     +  &quot;n&quot;    ==&gt;  &quot;en&quot;
277:  &quot;&lt;&quot;     +  &quot;/&quot;    ==&gt;  &quot;&lt;/&quot;
278:  &quot;1&quot;     +  &quot;9&quot;    ==&gt;  &quot;19&quot;
279:  &quot;&amp;qu&quot;   +  &quot;o&quot;    ==&gt;  &quot;&amp;quo&quot;
280:  &quot;&amp;quo&quot;  +  &quot;t;&quot;   ==&gt;  &quot;&amp;quot;&quot;
...:  ...     +  ...    ==&gt;  ...</code></pre>
<!-- 1000: "&amp;nbsp"  +  ";"             ==>  "&amp;nbsp;" -->
<!-- 1001: "19"         +  "4"             ==>  "194" -->
<!-- 1002: "Ch"         +  "r"             ==>  "Chr" -->
<!-- 1003: "Chr"        +  "ist"           ==>  "Christ" -->
<!-- 1004: "Christ"     +  "ian"           ==>  "Christian" -->
<!-- 1005: "t"          +  "ain"           ==>  "tain" -->
<!-- 1006: "pro"        +  "v"             ==>  "prov" -->
<!-- 1007: "most"       +  " "             ==>  "most " -->
<!-- 1008: ". "         +  "It"            ==>  ". It" -->
<!-- 1009: "w"          +  "ell"           ==>  "well" -->
<!-- 1010: ".\n\n=="    +  "="             ==>  ".\n\n===" -->
<!-- 1011: "u"          +  "ally "         ==>  "ually " -->
<!-- 1012: "such"       +  " "             ==>  "such " -->
<!-- 1013: " the"       +  "m"             ==>  " them" -->
<!-- 1014: "a"          +  "th"            ==>  "ath" -->
<!-- 1015: "de"         +  "ath"           ==>  "death" -->
<!-- 1016: "\n*"        +  "[http://www."  ==>  "\n*[http://www." -->
<!-- 1017: "it"         +  "ies"           ==>  "ities" -->
<!-- 1018: "th"         +  "um"            ==>  "thum" -->
<!-- 1019: "|"          +  "thum"          ==>  "|thum" -->
<!-- ...   ...             ...                  ... --
<!-- 70000: "rop"         +  "e "           ==>  "rope " -->
<!-- 70001: "Clock"       +  "wise "        ==>  "Clockwise " -->
<!-- 70002: "Governor "   +  "       = [["  ==>  "Governor        = [[" -->
<!-- 70003: "dynamic "    +  "soar"         ==>  "dynamic soar" -->
<!-- 70004: "soft "       +  "mud"          ==>  "soft mud" -->
<!-- 70005: "button"      +  "s "           ==>  "buttons " -->
<!-- 70006: "aband"       +  "on "          ==>  "abandon " -->
<!-- 70007: "phot"        +  "on "          ==>  "photon " -->
<!-- 70008: "os"          +  "is"           ==>  "osis" -->
<!-- 70009: "Azerbaijan"  +  "is"           ==>  "Azerbaijanis" -->
<!-- 70010: "is"          +  "lav"          ==>  "islav" -->
<!-- 70011: "galax"       +  "ies"          ==>  "galaxies" -->
<!-- 70012: "galax"       +  "y"            ==>  "galaxy" -->
<!-- 70013: "economy "    +  "is "          ==>  "economy is " -->
<!-- 70014: "Arthrit"     +  "is "          ==>  "Arthritis " -->
<p>Full output (CSV): <a href="res/chunk/out/enwik7.csv"><code>enwik7</code></a>.</p>
<p>Compare these chunks to those that result from a naive combination
strategy where the most frequent joint is combined into a new symbol
(e.g. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Re-Pair">Re-Pair</a>). The produced
chunks are shorter and marginally less meaningful, at least at the
start:</p>
<pre><code>256: &quot;e&quot;   +  &quot; &quot;   ==&gt;  &quot;e &quot;
257: &quot;t&quot;   +  &quot;h&quot;   ==&gt;  &quot;th&quot;
258: &quot;s&quot;   +  &quot; &quot;   ==&gt;  &quot;s &quot;
259: &quot;e&quot;   +  &quot;r&quot;   ==&gt;  &quot;er&quot;
260: &quot;i&quot;   +  &quot;n&quot;   ==&gt;  &quot;in&quot;
261: &quot;a&quot;   +  &quot;n&quot;   ==&gt;  &quot;an&quot;
262: &quot;]&quot;   +  &quot;]&quot;   ==&gt;  &quot;]]&quot;
263: &quot;[&quot;   +  &quot;[&quot;   ==&gt;  &quot;[[&quot;
264: &quot;d&quot;   +  &quot; &quot;   ==&gt;  &quot;d &quot;
265: &quot;o&quot;   +  &quot;n&quot;   ==&gt;  &quot;on&quot;
266: &quot;,&quot;   +  &quot;  &quot;  ==&gt;  &quot;, &quot;
267: &quot;t&quot;   +  &quot; &quot;   ==&gt;  &quot;t &quot;
268: &quot;o&quot;   +  &quot;r&quot;   ==&gt;  &quot;or&quot;
269: &quot;th&quot;  +  &quot;e &quot;  ==&gt;  &quot;the &quot;
270: &quot;e&quot;   +  &quot;n&quot;   ==&gt;  &quot;en&quot;
271: &quot;t&quot;   +  &quot;i&quot;   ==&gt;  &quot;ti&quot;
272: &quot;a&quot;   +  &quot;r&quot;   ==&gt;  &quot;ar&quot;
273: &quot;a&quot;   +  &quot;l&quot;   ==&gt;  &quot;al&quot;
274: &quot; &quot;   +  &quot; &quot;   ==&gt;  &quot;  &quot;
275: &quot;o&quot;   +  &quot;f&quot;   ==&gt;  &quot;of&quot;
276: &quot;y&quot;   +  &quot; &quot;   ==&gt;  &quot;y &quot;
277: &quot;of&quot;  +  &quot; &quot;   ==&gt;  &quot;of &quot;
278: &quot;r&quot;   +  &quot;e&quot;   ==&gt;  &quot;re&quot;
279: &quot;s&quot;   +  &quot;t&quot;   ==&gt;  &quot;st&quot;
280: &quot;e&quot;   +  &quot;d &quot;  ==&gt;  &quot;ed &quot;
...: ...   +  ...   ==&gt;  ...</code></pre>
<!-- 1000: "w"     +  "ould "  ==>  "would " -->
<!-- 1001: "op"    +  "h"      ==>  "oph" -->
<!-- 1002: "2"     +  "5"      ==>  "25" -->
<!-- 1003: "p"     +  "or"     ==>  "por" -->
<!-- 1004: "i"     +  " "      ==>  "i " -->
<!-- 1005: "ic"    +  "t"      ==>  "ict" -->
<!-- 1006: "pro"   +  "duc"    ==>  "produc" -->
<!-- 1007: "M"     +  "ar"     ==>  "Mar" -->
<!-- 1008: "y"     +  "p"      ==>  "yp" -->
<!-- 1009: "\n\n"  +  "=="     ==>  "\n\n==" -->
<!-- 1010: "er"    +  "n "     ==>  "ern " -->
<!-- 1011: "l"     +  "arg"    ==>  "larg" -->
<!-- 1012: "gu"    +  "st"     ==>  "gust" -->
<!-- 1013: "er"    +  "\"      ==>  ", er\, " -->
<!-- 1014: "S"     +  "e"      ==>  "Se" -->
<!-- 1015: "c"     +  "ap"     ==>  "cap" -->
<!-- 1016: "[["    +  "P"      ==>  "[[P" -->
<!-- 1017: "er "   +  "of "    ==>  "er of " -->
<!-- 1018: "oug"   +  "h"      ==>  "ough" -->
<!-- 1019: "in"    +  "to "    ==>  "into " -->
<!-- ...   ...        ...           ... -->
<!-- 70000: "Build"        +  "ings and "  ==>  "Buildings and " -->
<!-- 70001: "fronti"       +  "er"         ==>  "frontier" -->
<!-- 70002: "Cypr"         +  "us]] "      ==>  "Cyprus]] " -->
<!-- 70003: "description "  +  "| url=http://www."  ==>  "description | url=http://www." -->
<!-- 70004: "ρι"           +  "στο�"      ==>  "ριστο�" -->
<!-- 70005: "Characters in Atlas_Shrugged" + "|" ==> "Characters in Atlas_Shrugged|" -->
<!-- 70006: "homosexual "  +  "sex is "    ==>  "homosexual sex is " -->
<!-- 70007: "Benjamin "    +  "Tuck"       ==>  "Benjamin Tuck" -->
<!-- 70008: "in [[leap year]]s"  +  ")\"  ==>  "with , in [[leap year]]s)\, with " -->
<!-- 70009: "Ba"           +  "ad"         ==>  "Baad" -->
<!-- 70010: "Ele"          +  "azar"       ==>  "Eleazar" -->
<!-- 70011: "Fres"         +  "nel,"       ==>  "Fresnel," -->
<!-- 70012: "UR"           +  "PS "        ==>  "URPS " -->
<!-- 70013: "dop"          +  "e.com/"     ==>  "dope.com/" -->
<!-- 70014: "eight"        +  "-"          ==>  "eight-" -->
<!-- 70015: "gosp"         +  "el"         ==>  "gospel" -->
<!-- 70016: "march"        +  "ed to "     ==>  "marched to " -->
<!-- 70017: "tum"          +  "or"         ==>  "tumor" -->
<!-- 70018: "tum"          +  "ul"         ==>  "tumul" -->
<!-- 70019: "imprison"     +  "ment"       ==>  "imprisonment" -->
<!-- 70020: "imprison"     +  "ment "      ==>  "imprisonment " -->
<!-- 70021: "resembl"      +  "ing "       ==>  "resembling " -->
<!-- 70022: "his father "  +  "had "       ==>  "his father had " -->
<!-- 70023: "persec"       +  "ution of "  ==>  "persecution of " -->
<!-- 70024: "trag"         +  "edy "       ==>  "tragedy " -->
<p>Full output (CSV): <a href="res/chunk/out/enwik7-naive-loss.csv"><code>enwik7-naive-loss</code></a>.</p>
<p>Naive chunks visibly have a bias towards combining symbols with high
occurrences even if the combination doesn’t hold much more meaning than
the sum of its parts. For example compared to the more meaningful</p>
<pre><code>&quot;&gt;\n&quot;, &quot;&lt;/&quot;, &quot;ing&quot;, &quot;&amp;quot;&quot;,</code></pre>
<p>the following chunks have more occurrences and are therefore selected
earlier by the naive policy:</p>
<pre><code>&quot;e &quot;, &quot;s &quot;, &quot;d &quot;, &quot;t &quot;.</code></pre>
<p>In terms of probability:</p>
<p><span class="math display">\[p(s_0,s_1) \sim p(s_0)p(s_1)\]</span></p>
<p>even when the two symbols are independent.</p>
<p>Instead, if we are interested in how much a joint occurs <em>relative</em> to a
null hypothesis of independence, we get a ratio:</p>
<p><span class="math display">\[\frac{p(s_0,s_1)}{p(s_0)p(s_1)}\]</span></p>
<p>which, in information terms, is known as the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">pointwise mutual
information
(PMI)</a>:</p>
<p><span class="math display">\[\begin{align} \mathrm{pmi}(s_0;s_1)
&amp;= \log\left(\frac{p(s_0,s_1)}{p(s_0)p(s_1)}\right) \\[5pt]
&amp;= \log\left(\frac{n_{01} \cdot N \cdot N}{N \cdot n_0 \cdot n_1}\right) \\[5pt]
&amp;= \log\left(\frac{n_{01} \, N}{n_0 \, n_1}\right) \\
\end{align}\]</span></p>
<p>Using this to score joint candidates, produces the somewhat degenerate
dictionary containing rare byte pairs that occur almost exclusively
together:</p>
<pre><code>256: &quot;�&quot;   +  &quot;�&quot;  ==&gt;  &quot;ی&quot;
257: &quot;�&quot;   +  &quot;�&quot;  ==&gt;  &quot;Ә&quot;
258: &quot;�&quot;   +  &quot;�&quot;  ==&gt;  &quot;ے&quot;
259: &quot;@&quot;    +  &quot;@&quot;   ==&gt;  &quot;@@&quot;
260: &quot;@@&quot;   +  &quot;@@&quot;  ==&gt;  &quot;@@@@&quot;
261: &quot;@@@@&quot; +  &quot;@@&quot;  ==&gt;  &quot;@@@@@@&quot;
262: &quot;@@@@&quot; +  &quot;@&quot;   ==&gt;  &quot;@@@@@&quot;
263: &quot;@@&quot;   +  &quot;@&quot;   ==&gt;  &quot;@@@&quot;
264: &quot;�&quot;   +  &quot;�&quot;  ==&gt;  &quot;ґ&quot;
265: &quot;�&quot;   +  &quot;�&quot;  ==&gt;  &quot;��&quot;
...: ...    +  ...   ==&gt;  ...</code></pre>
<p>We scale this information by the joint count to obtain a function more
representative of <em>total</em> change in information:</p>
<p><span class="math display">\[\begin{align} \mathrm{spmi}(s_0;s_1)
&amp;~=~ n_{01} \cdot \, \mathrm{pmi}(s_0;s_1) \\[5pt]
&amp;~=~ n_{01} \cdot \, \log\left(\frac{n_{01} \, N}{n_0 \, n_1}\right),
\end{align}\]</span></p>
<p>which gives a dictionary starting with:</p>
<pre><code>256:  &quot;]&quot;     +  &quot;]&quot;    ==&gt;  &quot;]]&quot;
257:  &quot;[&quot;     +  &quot;[&quot;    ==&gt;  &quot;[[&quot;
258:  &quot;t&quot;     +  &quot;h&quot;    ==&gt;  &quot;th&quot;
259:  &quot;th&quot;    +  &quot;e&quot;    ==&gt;  &quot;the&quot;
260:  &quot;i&quot;     +  &quot;n&quot;    ==&gt;  &quot;in&quot;
261:  &quot;a&quot;     +  &quot;n&quot;    ==&gt;  &quot;an&quot;
262:  &quot;o&quot;     +  &quot;n&quot;    ==&gt;  &quot;on&quot;
263:  &quot;,&quot;     +  &quot; &quot;    ==&gt;  &quot;, &quot;
264:  &quot; &quot;     +  &quot;the&quot;  ==&gt;  &quot; the&quot;
265:  &quot;'&quot;     +  &quot;'&quot;    ==&gt;  &quot;''&quot;
266:  &quot; the&quot;  +  &quot; &quot;    ==&gt;  &quot; the &quot;
267:  &quot;o&quot;     +  &quot;f&quot;    ==&gt;  &quot;of&quot;
268:  &quot;an&quot;    +  &quot;d&quot;    ==&gt;  &quot;and&quot;
269:  &quot;e&quot;     +  &quot;r&quot;    ==&gt;  &quot;er&quot;
270:  &quot;e&quot;     +  &quot;n&quot;    ==&gt;  &quot;en&quot;
271:  &quot;o&quot;     +  &quot;r&quot;    ==&gt;  &quot;or&quot;
272:  &quot; &quot;     +  &quot;of&quot;   ==&gt;  &quot; of&quot;
273:  &quot;\n&quot;    +  &quot;*&quot;    ==&gt;  &quot;\n*&quot;
274:  &quot;a&quot;     +  &quot;r&quot;    ==&gt;  &quot;ar&quot;
275:  &quot;a&quot;     +  &quot;l&quot;    ==&gt;  &quot;al&quot;
276:  &quot;e&quot;     +  &quot;d&quot;    ==&gt;  &quot;ed&quot;
277:  &quot;in&quot;    +  &quot;g&quot;    ==&gt;  &quot;ing&quot;
278:  &quot;a&quot;     +  &quot;t&quot;    ==&gt;  &quot;at&quot;
279:  &quot;t&quot;     +  &quot;;&quot;    ==&gt;  &quot;t;&quot;
280:  &quot;&amp;&quot;     +  &quot;q&quot;    ==&gt;  &quot;&amp;q&quot;
...:  ...     +  ...    ==&gt;  ...</code></pre>
<p>Full output (CSV): <a href="res/chunk/out/enwik7-spmi-loss.csv"><code>enwik7-spmi-loss</code></a></p>
<p>which is much more similar to the dictionary obtained from our
informational loss function.</p>
<p>Perhaps surprisingly, the naive dictionary achieves levels of
compression comparable to ours, and the SPMI dictionary’s performance is
nearly identical to ours:</p>
<p><img src="res/chunk/plots/factors-functions.svg" /></p>
<p>Measuring the average word lengths across the evolution of the
dictionary produces a similar pattern:</p>
<p><img src="res/chunk/plots/wl-functions.svg" /></p>
<p>where the sudden increase in word length occurring between introductions
400-1000 is driven by the discovery of strings common in all XML headers
of pages in the dataset (including redirects) like the <code>&lt;revision&gt;</code>,
<code>&lt;id&gt;</code> and <code>&lt;contributor&gt;</code> tags (and their indentations) producing
words of ~30 bytes long.</p>
<p>We also compute for each pair of methods discussed, a coefficient of
overlap</p>
<p><span class="math display">\[\frac{|\mathrm{\bf r}_a \cap \mathrm{\bf r}_b|}{|\mathrm{\bf r}_a|}
~~~~~~~ \mathrm{where} ~~ |\mathrm{\bf r}_a| = |\mathrm{\bf r}_b|\]</span></p>
<p>between the dictionaries:</p>
<p><img src="res/chunk/plots/overlap-functions.svg" /></p>
<p>showing the scaled PMI to produce dictionaries between the naive
approach and ours, but slightly closer to ours.</p>
<h2 id="progressive-sampling">Progressive Sampling</h2>
<p>In its current state, the program cannot process the larger strings
<code>enwik8</code> (100MB) or <code>enwik9</code> (1GB) on 16GB of RAM without thrashing
because of the large amount of bookkeeping that was implemented to speed
up execution.</p>
<p>Unfortunately, there seems to be little value in stretching smaller
samples to model the compression of larger strings. For example, simply
scaling the collected statistics—assuming sample homogeneity—breaks
down in the tail of the execution resulting in poor chunk choices the
closer we get to joint counts of 1, in some sort of reverse law of large
numbers.</p>
<p>Consider the factors achieved on the compression (eval.) of a large
string (<code>enwik7</code>) given dictionaries derived from (trained on) shorter
strings:</p>
<p><img src="res/chunk/plots/factors-eval-enwik7.svg" /></p>
<p>which is simply the previously shown <a href="#factors">compression factor
graph</a> with worse performance for each dataset smaller than
<code>enwik7</code>. Notice that the lines don’t taper off because keeping track of
the code length of a smaller string, we interrupt the derivation of the
dictionaries earlier than would have been optimal for the larger string.</p>
<p>The gap in performance worsens further into the execution we go, but
early in the execution, the difference is relatively modest. The overlap
is also significant:</p>
<p><img src="res/chunk/plots/overlap.svg" /></p>
<p>Could the key to processing very large strings be to <em>subsample</em> the
statistics, at least at the beginning of the execution, maintaining a
smaller, manageable, but still statistically significant sample? Since
the working string shrinks in size over the course of execution,
especially for larger ones:</p>
<p><img src="res/chunk/plots/str-len.svg" /></p>
<p>we could maintain a string of a <em>fixed size</em> and append more symbols as
the introduction of new construction rules shrinks it. It would have the
added benefit of keeping the memory profile somewhat constant instead of
heavy at the beginning and light at the end.</p>
<p>The drawback is that appending symbols adds not only to the joint
counts, but also to the (marginal) counts, changing the loss of each
candidate having that symbol as one of its parts, invalidating entries
in <a href="#joint-loss-bookkeeping">the loss map</a> that would otherwise preserve
between iterations. Depending on the symbols appended, this can be a
significant proportion of the losses that require getting re-indexed.</p>
<p>We compare the effectiveness of the dictionaries resulting from
progressive sampling with those of same starting lengths, but fixed, in
compressing a same larger string.</p>
<p>The effectiveness is somewhat lost on small strings, producing
marginally better factors than the fixed case:</p>
<p><img src="res/chunk/plots/factors-enwik4-5-eval-enwik5.svg" /></p>
<p><img src="res/chunk/plots/factors-enwik5-6-eval-enwik6.svg" /></p>
<p>But shows promise with large enough scale:</p>
<p><img src="res/chunk/plots/factors-enwik6-7-eval-enwik7.svg" /></p>
<p>Unfortunately, as the size of the sample (<span class="math inline">\(N\)</span>) and dictionary (<span class="math inline">\(m\)</span>)
increases, so does the number of candidates (<span class="math inline">\(O(\mathrm{min}(N,m^2))\)</span>),
and with it, the cost associated with re-indexing the invalidated losses
in the loss map.</p>
<p>This one operation quickly takes a majority share of the
computation-time and, for all but the smallest strings, makes simply
starting with the larger string the more time efficient alternative:</p>
<pre><code>         Data           Time (relative)

enwik4                        1.0
enwik4-5 (progressive)        3.3
enwik5                        4.9
enwik5-6 (progressive)       77.3
enwik6                       66.3
enwik6-7 (progressive)     2868.2
enwik7                     1488.6</code></pre>
<p>which demotes the method from a potential entry into processing much
larger strings into one that only shifts part of the space complexity to
time.</p>
  </section>
  <footer>
    <a href="./">Return</a>
  </footer>
</article>

    </main>

  </body>
</html>
